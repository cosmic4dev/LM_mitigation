{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch\n",
        "!pip install transformers accelerate\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import ElectraForTokenClassification, ElectraTokenizerFast, AdamW, get_scheduler\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgUM0KUGBLg1",
        "outputId": "cadee097-2e5d-4f62-d724-2a2c2d29b3e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "ds = load_dataset(\"humane-lab/K-HATERS\")\n",
        "\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "# Tokenizer를 기반으로 DataCollator 생성 (다이나믹 패딩 적용)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 변경 방식 with BIO tag - try1\n",
        "# def create_bio_labels(texts, rationales, tokenizer):\n",
        "#     tokenized_texts = tokenizer(\n",
        "#         texts, truncation=True, padding=True, return_offsets_mapping=True\n",
        "#     )\n",
        "#     labels = []\n",
        "\n",
        "#     for i, (text, rationale_spans) in enumerate(zip(texts, rationales)):\n",
        "#         # 모든 토큰에 'O'로 일단 초기화\n",
        "#         token_labels = [\"O\"] * len(tokenized_texts[\"input_ids\"][i])\n",
        "\n",
        "#         # 유해 스팬과 오프셋 매핑 비교\n",
        "#         for span in rationale_spans:\n",
        "#             start, end = span  # 유해 스팬의 시작과 끝\n",
        "#             for idx, (offset_start, offset_end) in enumerate(tokenized_texts[\"offset_mapping\"][i]):\n",
        "#                 if offset_start >= start and offset_end <= end:\n",
        "#                     if token_labels[idx] == \"O\":\n",
        "#                         token_labels[idx] = \"B\"\n",
        "#                     else:\n",
        "#                         token_labels[idx] = \"I\"\n",
        "\n",
        "#         labels.append(token_labels)\n",
        "\n",
        "#     # offset_mapping은 학습에 필요 없으므로 제거\n",
        "#     tokenized_texts.pop(\"offset_mapping\")\n",
        "#     tokenized_texts[\"labels\"] = labels\n",
        "#     return tokenized_texts\n",
        "\n",
        "# try 2\n",
        "# def create_bio_labels(texts, rationales, tokenizer):\n",
        "#     tokenized_texts = tokenizer(\n",
        "#         texts, truncation=True, padding=True, return_offsets_mapping=True\n",
        "#     )\n",
        "#     labels = []\n",
        "\n",
        "#     for i, (text, rationale_spans) in enumerate(zip(texts, rationales)):\n",
        "#         # 모든 토큰에 'O'로 초기화\n",
        "#         token_labels = [\"O\"] * len(tokenized_texts[\"input_ids\"][i])\n",
        "#         previous_tag = \"O\"  # 이전 토큰의 태그 상태를 저장\n",
        "\n",
        "#         for span in rationale_spans:\n",
        "#             start, end = span  # 유해 스팬의 시작과 끝\n",
        "#             for idx, (offset_start, offset_end) in enumerate(tokenized_texts[\"offset_mapping\"][i]):\n",
        "#                 # Special tokens ([CLS], [SEP] 등)는 태깅하지 않음\n",
        "#                 if offset_start == 0 and offset_end == 0:\n",
        "#                     continue\n",
        "\n",
        "#                 # 유해 스팬과 오프셋 비교\n",
        "#                 if offset_start >= start and offset_end <= end:\n",
        "#                     # B와 I 태깅 구분\n",
        "#                     if previous_tag in [\"O\", \"B\"]:  # 이전에 'O'거나 새로운 시작이면 'B'\n",
        "#                         token_labels[idx] = \"B\"\n",
        "#                     else:\n",
        "#                         token_labels[idx] = \"I\"\n",
        "#                     previous_tag = token_labels[idx]\n",
        "#                 else:\n",
        "#                     previous_tag = \"O\"  # 범위 바깥이면 초기화\n",
        "\n",
        "#         labels.append(token_labels)\n",
        "\n",
        "#     # offset_mapping은 학습에 필요 없으므로 제거\n",
        "#     tokenized_texts.pop(\"offset_mapping\")\n",
        "#     tokenized_texts[\"labels\"] = labels\n",
        "#     return tokenized_texts\n",
        "\n",
        "# try3\n",
        "def create_bio_labels(texts, rationales, tokenizer):\n",
        "    tokenized_texts = tokenizer(\n",
        "        texts, truncation=True, padding=True, return_offsets_mapping=True\n",
        "    )\n",
        "    labels = []\n",
        "\n",
        "    for i, (text, rationale_spans) in enumerate(zip(texts, rationales)):\n",
        "        # 모든 토큰에 'O'로 초기화\n",
        "        token_labels = [\"O\"] * len(tokenized_texts[\"input_ids\"][i])\n",
        "        previous_tag = \"O\"  # 이전 태그 상태를 저장\n",
        "\n",
        "        for span in rationale_spans:\n",
        "            start, end = span  # 유해 스팬의 시작과 끝\n",
        "            for idx, (offset_start, offset_end) in enumerate(tokenized_texts[\"offset_mapping\"][i]):\n",
        "                # Special tokens ([CLS], [SEP] 등)는 태깅하지 않음\n",
        "                if offset_start == 0 and offset_end == 0:\n",
        "                    continue\n",
        "\n",
        "                # 유해 스팬과 오프셋 비교\n",
        "                if offset_start >= start and offset_end <= end:\n",
        "                    # B와 I 태깅 구분\n",
        "                    if token_labels[idx] == \"O\":\n",
        "                        if previous_tag == \"O\":  # 이전 태그가 'O'이면 'B' 태깅\n",
        "                            token_labels[idx] = \"B\"\n",
        "                        else:  # 이전 태그가 'B' 또는 'I'이면 'I' 태깅\n",
        "                            token_labels[idx] = \"I\"\n",
        "                    previous_tag = token_labels[idx]\n",
        "                else:\n",
        "                    previous_tag = \"O\"  # 범위 바깥이면 초기화\n",
        "\n",
        "        labels.append(token_labels)\n",
        "\n",
        "    # offset_mapping은 학습에 필요 없으므로 제거\n",
        "    tokenized_texts.pop(\"offset_mapping\")\n",
        "    tokenized_texts[\"labels\"] = labels\n",
        "    return tokenized_texts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# BIO 레이블 데이터 생성\n",
        "train_data = create_bio_labels(ds['train']['text'], ds['train']['offensiveness_rationale'], tokenizer)\n",
        "validation_data = create_bio_labels(ds['validation']['text'], ds['validation']['offensiveness_rationale'], tokenizer)\n",
        "test_data = create_bio_labels(ds['test']['text'], ds['test']['offensiveness_rationale'], tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYDS2tLgBZAb",
        "outputId": "f8385fe5-d72e-4345-81be-cad6878e0f55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'ElectraTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'ElectraTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bio_mapping = {'O': 0, 'B': 1, 'I': 2}\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long),\n",
        "            'labels': torch.tensor([bio_mapping[label] for label in self.encodings['labels'][idx]], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# # 학습 데이터셋 생성 to pytorch\n",
        "train_dataset = CustomDataset(train_data)\n",
        "validation_dataset = CustomDataset(validation_data)\n",
        "test_dataset = CustomDataset(test_data)\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)  # 동적 패딩 적용\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=16, collate_fn=data_collator)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "JQWdjPk0B9hx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 확인해보자.\n",
        "for i in range(1, 100, 3):\n",
        "    print(\"Text:\", ds['train']['text'][i])\n",
        "    print(\"Labels:\", train_data['labels'][i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZEWWt_QCCcq",
        "outputId": "de71690e-30f8-441c-b5e9-3cfee4b3abf1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 120시간 발언때도 마치 근로자들 120시간을 강제노역 시키자는 얘기로 왜곡하고 기사를 쓰더니~ 이번에도 무슨 서민들 불량식품 먹이자는 내용으로 왜곡하는구나~ 참~ 수준이~~\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 우리는 이런 민족이고 남 어려운일 있으면 츤데레처럼 일단 도와주고 봤었는데 정치에서 시작된 지역간 분쟁 세대간을 넘어 페미라며 남녀분쟁까지 이 모든걸 조장하는 정치를 하는 사람들 때려주고싶다\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 네이버 개혁하라 말못하는 #@이름# #@이름# 성적조작하는 #@이름# #@이름#돈맛을 아는 장모 #@이름#왜 비교되지??\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: ㅅㅂ 틀딱세끼야. 범죄자를 옹호하는거보니 너도 대통령하면 안되겠다. 너같은게 대통령되면 나라가 썩어빠지는거야.\n",
            "Labels: ['O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: #@이름# 후손들 부끄럽게 만들지 말아라.\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: #@이름# 지지해주는 노이즈 마켓팅인데? 이러면 한국국민들이 열받고 더 #@이름#을 지지해주는 걸 만드는 거의 북한 총풍수준인데. #@이름#가 #@이름#랑 뭐 있나?\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 제발 전문가세게 맡기고 가만있으라.\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 싹다 쏴죽여라 니미 개주제에\n",
            "Labels: ['O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 20대 여자 개돼지 표본들의 표따위 필요없다 나머지세대가 밀어주면그만.\n",
            "Labels: ['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 반성합니다#@이름#과 180석 뽑아준 국민들 잘못이 큽니다 그권력을 방패삼아 저런 기생충 바퀴벌레들이 호의소식 하며 권력에 기생하는거 아닙니까 진짜 투표 잘합시다\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 패널로 나와 #@이름#이 죽도록 빨아 대더니 결국엔 한자리 하네 끼리끼리 해도해도 너무한다\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 관종.. 능력이 안 되니 딴걸로 이슈를.. 쯧쯧\n",
            "Labels: ['O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 제잘난 맛에 천방지축 뛰는 철없는 당대표 뽑아놨다가 이 무슨 난장판인가. 이 정도면 민주당이 심은 세작이라해도 손색 없겠다. 역사에 죄를 짓지 말고 제발 성숙하게 행동하라.\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 한녀들원래 부러우면 이렇게댓글다는거냐?ㅋㅋ\n",
            "Labels: ['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 사람의 바닥은 금방 보인다. #@이름#이 그랬다. 인품과 깊이, 그 사람의 가치관은 숨기기 힘들다.\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 내로남불 철면피 더불성범죄당 #@이름# 패거리들이 내년 대선에서 #@이름# 국힘당에 정권 넘어가면 그동안 저질은 밝혀진 숨겨졌던 비리들로 인해 교도소 갈 일에 걱정이 태산 4월부터 국민 몰래 북독재자에게 #@이름# #@이름#을 시켜 서신을 보내 위대한 #@이름# 동지 우리 또한번 보여주기 평화쑈 합시다 백신과 생필품보내줄테니 비핵화 하는척 분위기 만들면 우리가 또 정권잡고 그대한테 유리할테니 우선 통신선 복원해주면 남북연락사무소 폭파 공무원살해 묻지않고 한미연합훈련 취소할게요 제발 나좀 살려주오 감방가게 생겼소 으잉 동생아ㅡ\n",
            "Labels: ['O', 'B', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 인간아 왜사니 ~~\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 우리나라를 위해서 도와주셨으니 우리도 그의 가족까지 도와야죠\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 코로나 극심인데 대규모 콘서트? 양심 있어봐라 나씨 ㅠ\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 굶어보고 반론제기하는겨? 일주일 물막먹고 견뎌봐! 부정식품? 불랑식품도 없어서 못먹어! 배부른 소리하네. 전세계 기아가 왜 있는대.\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 대깨 원조는 한국인데 이것들또따라하네 대깨 원조는 대깨문이다 따라하지마라\n",
            "Labels: ['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 성적 모욕에 해당하는 #@이름# 건과 #@이름# 건을 동일시하는 논리 구조에 참 억지도 세상 이런 억지가 없구나 싶네ㅋㅋ\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 역시 자화자찬의 대가이십니다.\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 아예 도시를 폐허로 만들었으면 더 좋았을텐데. 더러운 짱깨들 청소할 좋은 기회를 놓쳤네..\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 살아있음에 감사해야하나 싶다 ㅠㅠ\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 중국인은 보이는대로 쳐 죽여야합니다\n",
            "Labels: ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 개가 전혀 저 어머니를 보호 한 거 같지 않은데개가 옆에 갔다고 보호 ㅡㅡ 냉무\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 역시 진정한 삺. 기치있는 삶을 사시는군요. 쫌 배우세요. 해골찬.그 518아?518명단 공개 해야 된다.운동권이 망친 나라에 이런운동하신분도 있구나?\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 그냥 따로 사시지.. 아들이 백수도 아니고 일 스스로 찾아하는데 뭔 잔소리를 그리도..\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 저~ 범죄자놈은 교도소근무자님들 짜증나니까 ~ 그럴때마다 이층에서 던져버리세요 왜 짜증나니까 ~ 에이 ??새끼\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: #@이름#아 너는 청와대서 떠나지 마라.녜뇸이 돌아 다니면 #@이름#만 몰고 다닌다.청와대만 잘지켜라\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 강아지는 풀어주고 #@이름#이나 대신 집어넣어라\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: 요즘 중동에서 여자로 태어나지 않았음에 감사드리게 된다.\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블이 잘 안 붙는다."
      ],
      "metadata": {
        "id": "Q52nZuduCFY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디버깅\n",
        "# 오프셋과 토큰 확인\n",
        "for i in range(3):\n",
        "    tokenized = tokenizer(ds[\"train\"][\"text\"][i], return_offsets_mapping=True)\n",
        "    print(f\"Text: {ds['train']['text'][i]}\")\n",
        "    print(\"Tokens and Offsets:\")\n",
        "    for token, offset in zip(\n",
        "        tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"]),\n",
        "        tokenized[\"offset_mapping\"]\n",
        "    ):\n",
        "        print(f\"Token: {token}, Offset: {offset}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEDKQTwsHX-d",
        "outputId": "5bf63361-3539-488a-900b-f27756147ed9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 하나도 모르는 얼라 쉭 끼가 설치는꼬라서니가 무릇 텅빈 백 정한테 칼자루 쥐여준 형국 민 좃 당 애 세들 속은 닐리리 맘보 통탄스럽구나 나라의 흥망성쇠 갈림길에 저런 등 신 들이 관여 한다는것이\n",
            "Tokens and Offsets:\n",
            "Token: [CLS], Offset: (0, 0)\n",
            "Token: 하나도, Offset: (0, 3)\n",
            "Token: 모르는, Offset: (4, 7)\n",
            "Token: 얼, Offset: (8, 9)\n",
            "Token: ##라, Offset: (9, 10)\n",
            "Token: 쉭, Offset: (11, 12)\n",
            "Token: 끼, Offset: (13, 14)\n",
            "Token: ##가, Offset: (14, 15)\n",
            "Token: 설치는, Offset: (16, 19)\n",
            "Token: ##꼬, Offset: (19, 20)\n",
            "Token: ##라서, Offset: (20, 22)\n",
            "Token: ##니가, Offset: (22, 24)\n",
            "Token: 무릇, Offset: (25, 27)\n",
            "Token: 텅, Offset: (28, 29)\n",
            "Token: ##빈, Offset: (29, 30)\n",
            "Token: 백, Offset: (31, 32)\n",
            "Token: 정한, Offset: (33, 35)\n",
            "Token: ##테, Offset: (35, 36)\n",
            "Token: 칼, Offset: (37, 38)\n",
            "Token: ##자루, Offset: (38, 40)\n",
            "Token: 쥐, Offset: (41, 42)\n",
            "Token: ##여, Offset: (42, 43)\n",
            "Token: ##준, Offset: (43, 44)\n",
            "Token: 형국, Offset: (45, 47)\n",
            "Token: 민, Offset: (48, 49)\n",
            "Token: 좃, Offset: (50, 51)\n",
            "Token: 당, Offset: (52, 53)\n",
            "Token: 애, Offset: (54, 55)\n",
            "Token: 세, Offset: (56, 57)\n",
            "Token: ##들, Offset: (57, 58)\n",
            "Token: 속은, Offset: (59, 61)\n",
            "Token: 닐, Offset: (62, 63)\n",
            "Token: ##리, Offset: (63, 64)\n",
            "Token: ##리, Offset: (64, 65)\n",
            "Token: 맘, Offset: (66, 67)\n",
            "Token: ##보, Offset: (67, 68)\n",
            "Token: 통탄, Offset: (69, 71)\n",
            "Token: ##스럽, Offset: (71, 73)\n",
            "Token: ##구나, Offset: (73, 75)\n",
            "Token: 나라의, Offset: (76, 79)\n",
            "Token: 흥, Offset: (80, 81)\n",
            "Token: ##망, Offset: (81, 82)\n",
            "Token: ##성, Offset: (82, 83)\n",
            "Token: ##쇠, Offset: (83, 84)\n",
            "Token: 갈림길, Offset: (85, 88)\n",
            "Token: ##에, Offset: (88, 89)\n",
            "Token: 저런, Offset: (90, 92)\n",
            "Token: 등, Offset: (93, 94)\n",
            "Token: 신, Offset: (95, 96)\n",
            "Token: 들이, Offset: (97, 99)\n",
            "Token: 관여, Offset: (100, 102)\n",
            "Token: 한다는, Offset: (103, 106)\n",
            "Token: ##것이, Offset: (106, 108)\n",
            "Token: [SEP], Offset: (0, 0)\n",
            "--------------------------------------------------\n",
            "Text: 120시간 발언때도 마치 근로자들 120시간을 강제노역 시키자는 얘기로 왜곡하고 기사를 쓰더니~ 이번에도 무슨 서민들 불량식품 먹이자는 내용으로 왜곡하는구나~ 참~ 수준이~~\n",
            "Tokens and Offsets:\n",
            "Token: [CLS], Offset: (0, 0)\n",
            "Token: 120시간, Offset: (0, 5)\n",
            "Token: 발언, Offset: (6, 8)\n",
            "Token: ##때도, Offset: (8, 10)\n",
            "Token: 마치, Offset: (11, 13)\n",
            "Token: 근로자, Offset: (14, 17)\n",
            "Token: ##들, Offset: (17, 18)\n",
            "Token: 120시간, Offset: (19, 24)\n",
            "Token: ##을, Offset: (24, 25)\n",
            "Token: 강제, Offset: (26, 28)\n",
            "Token: ##노, Offset: (28, 29)\n",
            "Token: ##역, Offset: (29, 30)\n",
            "Token: 시키자, Offset: (31, 34)\n",
            "Token: ##는, Offset: (34, 35)\n",
            "Token: 얘기, Offset: (36, 38)\n",
            "Token: ##로, Offset: (38, 39)\n",
            "Token: 왜곡, Offset: (40, 42)\n",
            "Token: ##하고, Offset: (42, 44)\n",
            "Token: 기사를, Offset: (45, 48)\n",
            "Token: 쓰, Offset: (49, 50)\n",
            "Token: ##더니, Offset: (50, 52)\n",
            "Token: ~, Offset: (52, 53)\n",
            "Token: 이번에도, Offset: (54, 58)\n",
            "Token: 무슨, Offset: (59, 61)\n",
            "Token: 서민들, Offset: (62, 65)\n",
            "Token: 불량, Offset: (66, 68)\n",
            "Token: ##식품, Offset: (68, 70)\n",
            "Token: 먹이, Offset: (71, 73)\n",
            "Token: ##자는, Offset: (73, 75)\n",
            "Token: 내용, Offset: (76, 78)\n",
            "Token: ##으로, Offset: (78, 80)\n",
            "Token: 왜곡, Offset: (81, 83)\n",
            "Token: ##하는구나, Offset: (83, 87)\n",
            "Token: ~, Offset: (87, 88)\n",
            "Token: 참, Offset: (89, 90)\n",
            "Token: ~, Offset: (90, 91)\n",
            "Token: 수준이, Offset: (92, 95)\n",
            "Token: ~, Offset: (95, 96)\n",
            "Token: ~, Offset: (96, 97)\n",
            "Token: [SEP], Offset: (0, 0)\n",
            "--------------------------------------------------\n",
            "Text: 8?15참석자는 살인자라더니 이넘들한테는 \\유감을 표한다, 요청한다, ~해주시기 바란다\\ ??? 상전도 이런 상전이 없네\n",
            "Tokens and Offsets:\n",
            "Token: [CLS], Offset: (0, 0)\n",
            "Token: 8, Offset: (0, 1)\n",
            "Token: ?, Offset: (1, 2)\n",
            "Token: 15, Offset: (2, 4)\n",
            "Token: ##참, Offset: (4, 5)\n",
            "Token: ##석, Offset: (5, 6)\n",
            "Token: ##자는, Offset: (6, 8)\n",
            "Token: 살인자, Offset: (9, 12)\n",
            "Token: ##라더니, Offset: (12, 15)\n",
            "Token: 이넘, Offset: (16, 18)\n",
            "Token: ##들한테는, Offset: (18, 22)\n",
            "Token: \\, Offset: (23, 24)\n",
            "Token: 유감, Offset: (24, 26)\n",
            "Token: ##을, Offset: (26, 27)\n",
            "Token: 표, Offset: (28, 29)\n",
            "Token: ##한다, Offset: (29, 31)\n",
            "Token: ,, Offset: (31, 32)\n",
            "Token: 요청, Offset: (33, 35)\n",
            "Token: ##한다, Offset: (35, 37)\n",
            "Token: ,, Offset: (37, 38)\n",
            "Token: ~, Offset: (39, 40)\n",
            "Token: 해주, Offset: (40, 42)\n",
            "Token: ##시기, Offset: (42, 44)\n",
            "Token: 바란다, Offset: (45, 48)\n",
            "Token: \\, Offset: (48, 49)\n",
            "Token: ?, Offset: (50, 51)\n",
            "Token: ?, Offset: (51, 52)\n",
            "Token: ?, Offset: (52, 53)\n",
            "Token: 상전, Offset: (54, 56)\n",
            "Token: ##도, Offset: (56, 57)\n",
            "Token: 이런, Offset: (58, 60)\n",
            "Token: 상전, Offset: (61, 63)\n",
            "Token: ##이, Offset: (63, 64)\n",
            "Token: 없네, Offset: (65, 67)\n",
            "Token: [SEP], Offset: (0, 0)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BIO 태깅 확인 함수\n",
        "def debug_bio_tagging(texts, rationales, tokenizer, max_samples=5):\n",
        "    \"\"\"\n",
        "    BIO 태깅 결과 디버깅 함수.\n",
        "    - texts: 입력 텍스트 리스트\n",
        "    - rationales: 유해성 스팬 정보\n",
        "    \"\"\"\n",
        "    tokenized_texts = tokenizer(\n",
        "        texts, truncation=True, padding=True, return_offsets_mapping=True\n",
        "    )\n",
        "    for i, (text, rationale_spans) in enumerate(zip(texts, rationales)):\n",
        "        if i >= max_samples:  # 최대 샘플 수 초과 시 중단\n",
        "            break\n",
        "        print(f\"Text: {text}\")\n",
        "        print(\"Tokens, Offsets, and BIO Labels:\")\n",
        "\n",
        "        token_labels = [\"O\"] * len(tokenized_texts[\"input_ids\"][i])  # 초기화\n",
        "        for span in rationale_spans:\n",
        "            start, end = span\n",
        "            for idx, (offset_start, offset_end) in enumerate(tokenized_texts[\"offset_mapping\"][i]):\n",
        "                if offset_start >= start and offset_end <= end:\n",
        "                    if token_labels[idx] == \"O\":\n",
        "                        token_labels[idx] = \"B\"\n",
        "                    else:\n",
        "                        token_labels[idx] = \"I\"\n",
        "\n",
        "        tokens = tokenizer.convert_ids_to_tokens(tokenized_texts[\"input_ids\"][i])\n",
        "        offsets = tokenized_texts[\"offset_mapping\"][i]\n",
        "        for token, offset, label in zip(tokens, offsets, token_labels):\n",
        "            print(f\"Token: {token}, Offset: {offset}, Label: {label}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# BIO 태깅 디버깅 실행\n",
        "debug_bio_tagging(ds[\"train\"][\"text\"], ds[\"train\"][\"offensiveness_rationale\"], tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF7bfIzG1aYd",
        "outputId": "953cc5ba-62b5-46c7-eb5f-c2183f26c84c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 하나도 모르는 얼라 쉭 끼가 설치는꼬라서니가 무릇 텅빈 백 정한테 칼자루 쥐여준 형국 민 좃 당 애 세들 속은 닐리리 맘보 통탄스럽구나 나라의 흥망성쇠 갈림길에 저런 등 신 들이 관여 한다는것이\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 하나도, Offset: (0, 3), Label: O\n",
            "Token: 모르는, Offset: (4, 7), Label: O\n",
            "Token: 얼, Offset: (8, 9), Label: B\n",
            "Token: ##라, Offset: (9, 10), Label: B\n",
            "Token: 쉭, Offset: (11, 12), Label: B\n",
            "Token: 끼, Offset: (13, 14), Label: B\n",
            "Token: ##가, Offset: (14, 15), Label: O\n",
            "Token: 설치는, Offset: (16, 19), Label: O\n",
            "Token: ##꼬, Offset: (19, 20), Label: O\n",
            "Token: ##라서, Offset: (20, 22), Label: O\n",
            "Token: ##니가, Offset: (22, 24), Label: O\n",
            "Token: 무릇, Offset: (25, 27), Label: O\n",
            "Token: 텅, Offset: (28, 29), Label: O\n",
            "Token: ##빈, Offset: (29, 30), Label: O\n",
            "Token: 백, Offset: (31, 32), Label: O\n",
            "Token: 정한, Offset: (33, 35), Label: O\n",
            "Token: ##테, Offset: (35, 36), Label: O\n",
            "Token: 칼, Offset: (37, 38), Label: O\n",
            "Token: ##자루, Offset: (38, 40), Label: O\n",
            "Token: 쥐, Offset: (41, 42), Label: O\n",
            "Token: ##여, Offset: (42, 43), Label: O\n",
            "Token: ##준, Offset: (43, 44), Label: O\n",
            "Token: 형국, Offset: (45, 47), Label: O\n",
            "Token: 민, Offset: (48, 49), Label: O\n",
            "Token: 좃, Offset: (50, 51), Label: B\n",
            "Token: 당, Offset: (52, 53), Label: O\n",
            "Token: 애, Offset: (54, 55), Label: B\n",
            "Token: 세, Offset: (56, 57), Label: B\n",
            "Token: ##들, Offset: (57, 58), Label: O\n",
            "Token: 속은, Offset: (59, 61), Label: O\n",
            "Token: 닐, Offset: (62, 63), Label: O\n",
            "Token: ##리, Offset: (63, 64), Label: O\n",
            "Token: ##리, Offset: (64, 65), Label: O\n",
            "Token: 맘, Offset: (66, 67), Label: O\n",
            "Token: ##보, Offset: (67, 68), Label: O\n",
            "Token: 통탄, Offset: (69, 71), Label: O\n",
            "Token: ##스럽, Offset: (71, 73), Label: O\n",
            "Token: ##구나, Offset: (73, 75), Label: O\n",
            "Token: 나라의, Offset: (76, 79), Label: O\n",
            "Token: 흥, Offset: (80, 81), Label: O\n",
            "Token: ##망, Offset: (81, 82), Label: O\n",
            "Token: ##성, Offset: (82, 83), Label: O\n",
            "Token: ##쇠, Offset: (83, 84), Label: O\n",
            "Token: 갈림길, Offset: (85, 88), Label: O\n",
            "Token: ##에, Offset: (88, 89), Label: O\n",
            "Token: 저런, Offset: (90, 92), Label: O\n",
            "Token: 등, Offset: (93, 94), Label: B\n",
            "Token: 신, Offset: (95, 96), Label: B\n",
            "Token: 들이, Offset: (97, 99), Label: O\n",
            "Token: 관여, Offset: (100, 102), Label: O\n",
            "Token: 한다는, Offset: (103, 106), Label: O\n",
            "Token: ##것이, Offset: (106, 108), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: 120시간 발언때도 마치 근로자들 120시간을 강제노역 시키자는 얘기로 왜곡하고 기사를 쓰더니~ 이번에도 무슨 서민들 불량식품 먹이자는 내용으로 왜곡하는구나~ 참~ 수준이~~\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 120시간, Offset: (0, 5), Label: O\n",
            "Token: 발언, Offset: (6, 8), Label: O\n",
            "Token: ##때도, Offset: (8, 10), Label: O\n",
            "Token: 마치, Offset: (11, 13), Label: O\n",
            "Token: 근로자, Offset: (14, 17), Label: O\n",
            "Token: ##들, Offset: (17, 18), Label: O\n",
            "Token: 120시간, Offset: (19, 24), Label: O\n",
            "Token: ##을, Offset: (24, 25), Label: O\n",
            "Token: 강제, Offset: (26, 28), Label: O\n",
            "Token: ##노, Offset: (28, 29), Label: O\n",
            "Token: ##역, Offset: (29, 30), Label: O\n",
            "Token: 시키자, Offset: (31, 34), Label: O\n",
            "Token: ##는, Offset: (34, 35), Label: O\n",
            "Token: 얘기, Offset: (36, 38), Label: O\n",
            "Token: ##로, Offset: (38, 39), Label: O\n",
            "Token: 왜곡, Offset: (40, 42), Label: O\n",
            "Token: ##하고, Offset: (42, 44), Label: O\n",
            "Token: 기사를, Offset: (45, 48), Label: O\n",
            "Token: 쓰, Offset: (49, 50), Label: O\n",
            "Token: ##더니, Offset: (50, 52), Label: O\n",
            "Token: ~, Offset: (52, 53), Label: O\n",
            "Token: 이번에도, Offset: (54, 58), Label: O\n",
            "Token: 무슨, Offset: (59, 61), Label: O\n",
            "Token: 서민들, Offset: (62, 65), Label: O\n",
            "Token: 불량, Offset: (66, 68), Label: O\n",
            "Token: ##식품, Offset: (68, 70), Label: O\n",
            "Token: 먹이, Offset: (71, 73), Label: O\n",
            "Token: ##자는, Offset: (73, 75), Label: O\n",
            "Token: 내용, Offset: (76, 78), Label: O\n",
            "Token: ##으로, Offset: (78, 80), Label: O\n",
            "Token: 왜곡, Offset: (81, 83), Label: O\n",
            "Token: ##하는구나, Offset: (83, 87), Label: O\n",
            "Token: ~, Offset: (87, 88), Label: O\n",
            "Token: 참, Offset: (89, 90), Label: O\n",
            "Token: ~, Offset: (90, 91), Label: O\n",
            "Token: 수준이, Offset: (92, 95), Label: O\n",
            "Token: ~, Offset: (95, 96), Label: O\n",
            "Token: ~, Offset: (96, 97), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: 8?15참석자는 살인자라더니 이넘들한테는 \\유감을 표한다, 요청한다, ~해주시기 바란다\\ ??? 상전도 이런 상전이 없네\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 8, Offset: (0, 1), Label: O\n",
            "Token: ?, Offset: (1, 2), Label: O\n",
            "Token: 15, Offset: (2, 4), Label: O\n",
            "Token: ##참, Offset: (4, 5), Label: O\n",
            "Token: ##석, Offset: (5, 6), Label: O\n",
            "Token: ##자는, Offset: (6, 8), Label: O\n",
            "Token: 살인자, Offset: (9, 12), Label: O\n",
            "Token: ##라더니, Offset: (12, 15), Label: O\n",
            "Token: 이넘, Offset: (16, 18), Label: O\n",
            "Token: ##들한테는, Offset: (18, 22), Label: O\n",
            "Token: \\, Offset: (23, 24), Label: O\n",
            "Token: 유감, Offset: (24, 26), Label: O\n",
            "Token: ##을, Offset: (26, 27), Label: O\n",
            "Token: 표, Offset: (28, 29), Label: O\n",
            "Token: ##한다, Offset: (29, 31), Label: O\n",
            "Token: ,, Offset: (31, 32), Label: O\n",
            "Token: 요청, Offset: (33, 35), Label: O\n",
            "Token: ##한다, Offset: (35, 37), Label: O\n",
            "Token: ,, Offset: (37, 38), Label: O\n",
            "Token: ~, Offset: (39, 40), Label: O\n",
            "Token: 해주, Offset: (40, 42), Label: O\n",
            "Token: ##시기, Offset: (42, 44), Label: O\n",
            "Token: 바란다, Offset: (45, 48), Label: O\n",
            "Token: \\, Offset: (48, 49), Label: O\n",
            "Token: ?, Offset: (50, 51), Label: O\n",
            "Token: ?, Offset: (51, 52), Label: O\n",
            "Token: ?, Offset: (52, 53), Label: O\n",
            "Token: 상전, Offset: (54, 56), Label: O\n",
            "Token: ##도, Offset: (56, 57), Label: O\n",
            "Token: 이런, Offset: (58, 60), Label: O\n",
            "Token: 상전, Offset: (61, 63), Label: O\n",
            "Token: ##이, Offset: (63, 64), Label: O\n",
            "Token: 없네, Offset: (65, 67), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: 굿아이디어네~이런 머리를 자주 써야지이스라엘도 좋고 우리도 좋고\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 굿, Offset: (0, 1), Label: O\n",
            "Token: ##아이, Offset: (1, 3), Label: O\n",
            "Token: ##디어, Offset: (3, 5), Label: O\n",
            "Token: ##네, Offset: (5, 6), Label: O\n",
            "Token: ~, Offset: (6, 7), Label: O\n",
            "Token: 이런, Offset: (7, 9), Label: O\n",
            "Token: 머리를, Offset: (10, 13), Label: O\n",
            "Token: 자주, Offset: (14, 16), Label: O\n",
            "Token: 써야지, Offset: (17, 20), Label: O\n",
            "Token: ##이스, Offset: (20, 22), Label: O\n",
            "Token: ##라, Offset: (22, 23), Label: O\n",
            "Token: ##엘, Offset: (23, 24), Label: O\n",
            "Token: ##도, Offset: (24, 25), Label: O\n",
            "Token: 좋고, Offset: (26, 28), Label: O\n",
            "Token: 우리도, Offset: (29, 32), Label: O\n",
            "Token: 좋고, Offset: (33, 35), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: 우리는 이런 민족이고 남 어려운일 있으면 츤데레처럼 일단 도와주고 봤었는데 정치에서 시작된 지역간 분쟁 세대간을 넘어 페미라며 남녀분쟁까지 이 모든걸 조장하는 정치를 하는 사람들 때려주고싶다\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 우리는, Offset: (0, 3), Label: O\n",
            "Token: 이런, Offset: (4, 6), Label: O\n",
            "Token: 민족이, Offset: (7, 10), Label: O\n",
            "Token: ##고, Offset: (10, 11), Label: O\n",
            "Token: 남, Offset: (12, 13), Label: O\n",
            "Token: 어려운, Offset: (14, 17), Label: O\n",
            "Token: ##일, Offset: (17, 18), Label: O\n",
            "Token: 있으면, Offset: (19, 22), Label: O\n",
            "Token: 츤, Offset: (23, 24), Label: O\n",
            "Token: ##데, Offset: (24, 25), Label: O\n",
            "Token: ##레, Offset: (25, 26), Label: O\n",
            "Token: ##처럼, Offset: (26, 28), Label: O\n",
            "Token: 일단, Offset: (29, 31), Label: O\n",
            "Token: 도와주고, Offset: (32, 36), Label: O\n",
            "Token: 봤, Offset: (37, 38), Label: O\n",
            "Token: ##었는데, Offset: (38, 41), Label: O\n",
            "Token: 정치에, Offset: (42, 45), Label: O\n",
            "Token: ##서, Offset: (45, 46), Label: O\n",
            "Token: 시작된, Offset: (47, 50), Label: O\n",
            "Token: 지역, Offset: (51, 53), Label: O\n",
            "Token: ##간, Offset: (53, 54), Label: O\n",
            "Token: 분쟁, Offset: (55, 57), Label: O\n",
            "Token: 세대, Offset: (58, 60), Label: O\n",
            "Token: ##간을, Offset: (60, 62), Label: O\n",
            "Token: 넘어, Offset: (63, 65), Label: O\n",
            "Token: 페미, Offset: (66, 68), Label: O\n",
            "Token: ##라며, Offset: (68, 70), Label: O\n",
            "Token: 남녀, Offset: (71, 73), Label: O\n",
            "Token: ##분, Offset: (73, 74), Label: O\n",
            "Token: ##쟁, Offset: (74, 75), Label: O\n",
            "Token: ##까지, Offset: (75, 77), Label: O\n",
            "Token: 이, Offset: (78, 79), Label: O\n",
            "Token: 모든걸, Offset: (80, 83), Label: O\n",
            "Token: 조장하는, Offset: (84, 88), Label: O\n",
            "Token: 정치를, Offset: (89, 92), Label: O\n",
            "Token: 하는, Offset: (93, 95), Label: O\n",
            "Token: 사람들, Offset: (96, 99), Label: O\n",
            "Token: 때려, Offset: (100, 102), Label: B\n",
            "Token: ##주고, Offset: (102, 104), Label: O\n",
            "Token: ##싶다, Offset: (104, 106), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "Token: [PAD], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "오프셋 정보는 문제가 없어보임."
      ],
      "metadata": {
        "id": "_4RSOgghJDK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_check_bio_labels(dataset, tokenizer, processed_data, sample_size=5):\n",
        "    for _ in range(sample_size):\n",
        "        idx = random.randint(0, len(dataset[\"text\"]) - 1)  # 랜덤 인덱스 선택\n",
        "        text = dataset[\"text\"][idx]\n",
        "        rationale_spans = dataset[\"offensiveness_rationale\"][idx]\n",
        "        tokenized = tokenizer(text, truncation=True, padding=True, return_offsets_mapping=True)\n",
        "        labels = processed_data[\"labels\"][idx]\n",
        "\n",
        "        print(f\"Text: {text}\")\n",
        "        print(\"Tokens, Offsets, and BIO Labels:\")\n",
        "        for token, offset, label in zip(\n",
        "            tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"]),\n",
        "            tokenized[\"offset_mapping\"],\n",
        "            labels,\n",
        "        ):\n",
        "            print(f\"Token: {token}, Offset: {offset}, Label: {label}\")\n",
        "        print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "AupHes75JI2B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤으로 샘플 5개 확인\n",
        "random_check_bio_labels(\n",
        "    dataset=ds[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    processed_data=train_data,\n",
        "    sample_size=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9slfI8D3JMnX",
        "outputId": "22b08c6b-4399-4936-84eb-0ec391b14e5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 청와대? 청개구리들이 와글거리는 대가리 치매걸린 것들 모인곳?\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 청와대, Offset: (0, 3), Label: O\n",
            "Token: ?, Offset: (3, 4), Label: O\n",
            "Token: 청, Offset: (5, 6), Label: O\n",
            "Token: ##개, Offset: (6, 7), Label: O\n",
            "Token: ##구리, Offset: (7, 9), Label: O\n",
            "Token: ##들이, Offset: (9, 11), Label: O\n",
            "Token: 와, Offset: (12, 13), Label: O\n",
            "Token: ##글, Offset: (13, 14), Label: O\n",
            "Token: ##거리는, Offset: (14, 17), Label: O\n",
            "Token: 대가리, Offset: (18, 21), Label: B\n",
            "Token: 치매, Offset: (22, 24), Label: B\n",
            "Token: ##걸린, Offset: (24, 26), Label: O\n",
            "Token: 것들, Offset: (27, 29), Label: O\n",
            "Token: 모인, Offset: (30, 32), Label: O\n",
            "Token: ##곳, Offset: (32, 33), Label: O\n",
            "Token: ?, Offset: (33, 34), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: 우리가 우리나라 지키는 훈련하겠다는데 이래라 저래라 혀바닥 나불대지마라~~ 자금성 정문에다가 똥싸버리기전에\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 우리가, Offset: (0, 3), Label: O\n",
            "Token: 우리나라, Offset: (4, 8), Label: O\n",
            "Token: 지키는, Offset: (9, 12), Label: O\n",
            "Token: 훈련, Offset: (13, 15), Label: O\n",
            "Token: ##하겠다는, Offset: (15, 19), Label: O\n",
            "Token: ##데, Offset: (19, 20), Label: O\n",
            "Token: 이래라, Offset: (21, 24), Label: O\n",
            "Token: 저래라, Offset: (25, 28), Label: O\n",
            "Token: 혀, Offset: (29, 30), Label: O\n",
            "Token: ##바닥, Offset: (30, 32), Label: O\n",
            "Token: 나불대, Offset: (33, 36), Label: O\n",
            "Token: ##지마라, Offset: (36, 39), Label: O\n",
            "Token: ~, Offset: (39, 40), Label: O\n",
            "Token: ~, Offset: (40, 41), Label: O\n",
            "Token: 자금, Offset: (42, 44), Label: O\n",
            "Token: ##성, Offset: (44, 45), Label: O\n",
            "Token: 정문, Offset: (46, 48), Label: O\n",
            "Token: ##에다가, Offset: (48, 51), Label: O\n",
            "Token: 똥싸, Offset: (52, 54), Label: O\n",
            "Token: ##버리, Offset: (54, 56), Label: O\n",
            "Token: ##기전에, Offset: (56, 59), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: #@이름#씨, 당신은 아무리해도 대권경선에서도 안됩니다. 참 안쓰럽고 불쌍하다. 남의밥에 재뿌리기도 아니고~~ 자신이 안된다고 판을 뒤집어? 너같은 인간땜에 보수가 싸잡아 욕을 먹는거다. 넌 보수도 아니고 야당도 아니다. 그냥 OOO에 불과하다.\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: #, Offset: (0, 1), Label: O\n",
            "Token: @, Offset: (1, 2), Label: O\n",
            "Token: 이름, Offset: (2, 4), Label: O\n",
            "Token: #, Offset: (4, 5), Label: O\n",
            "Token: 씨, Offset: (5, 6), Label: O\n",
            "Token: ,, Offset: (6, 7), Label: O\n",
            "Token: 당신은, Offset: (8, 11), Label: O\n",
            "Token: 아무리, Offset: (12, 15), Label: O\n",
            "Token: ##해도, Offset: (15, 17), Label: O\n",
            "Token: 대권, Offset: (18, 20), Label: O\n",
            "Token: ##경선, Offset: (20, 22), Label: O\n",
            "Token: ##에서도, Offset: (22, 25), Label: O\n",
            "Token: 안됩니다, Offset: (26, 30), Label: O\n",
            "Token: ., Offset: (30, 31), Label: O\n",
            "Token: 참, Offset: (32, 33), Label: O\n",
            "Token: 안쓰, Offset: (34, 36), Label: O\n",
            "Token: ##럽고, Offset: (36, 38), Label: O\n",
            "Token: 불쌍하다, Offset: (39, 43), Label: O\n",
            "Token: ., Offset: (43, 44), Label: O\n",
            "Token: 남의, Offset: (45, 47), Label: O\n",
            "Token: ##밥, Offset: (47, 48), Label: O\n",
            "Token: ##에, Offset: (48, 49), Label: O\n",
            "Token: 재, Offset: (50, 51), Label: O\n",
            "Token: ##뿌리, Offset: (51, 53), Label: O\n",
            "Token: ##기도, Offset: (53, 55), Label: O\n",
            "Token: 아니고, Offset: (56, 59), Label: O\n",
            "Token: ~, Offset: (59, 60), Label: O\n",
            "Token: ~, Offset: (60, 61), Label: O\n",
            "Token: 자신이, Offset: (62, 65), Label: O\n",
            "Token: 안된다고, Offset: (66, 70), Label: O\n",
            "Token: 판을, Offset: (71, 73), Label: O\n",
            "Token: 뒤집어, Offset: (74, 77), Label: O\n",
            "Token: ?, Offset: (77, 78), Label: O\n",
            "Token: 너같은, Offset: (79, 82), Label: O\n",
            "Token: 인간, Offset: (83, 85), Label: O\n",
            "Token: ##땜에, Offset: (85, 87), Label: O\n",
            "Token: 보수가, Offset: (88, 91), Label: O\n",
            "Token: 싸잡아, Offset: (92, 95), Label: O\n",
            "Token: 욕을, Offset: (96, 98), Label: O\n",
            "Token: 먹는거, Offset: (99, 102), Label: O\n",
            "Token: ##다, Offset: (102, 103), Label: O\n",
            "Token: ., Offset: (103, 104), Label: O\n",
            "Token: 넌, Offset: (105, 106), Label: O\n",
            "Token: 보수도, Offset: (107, 110), Label: O\n",
            "Token: 아니고, Offset: (111, 114), Label: O\n",
            "Token: 야당도, Offset: (115, 118), Label: O\n",
            "Token: 아니다, Offset: (119, 122), Label: O\n",
            "Token: ., Offset: (122, 123), Label: O\n",
            "Token: 그냥, Offset: (124, 126), Label: O\n",
            "Token: OOO, Offset: (127, 130), Label: O\n",
            "Token: ##에, Offset: (130, 131), Label: O\n",
            "Token: 불과하다, Offset: (132, 136), Label: O\n",
            "Token: ., Offset: (136, 137), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: 이놈들은 말장난의 연금술사들이냐? 직접 머리 숙여 사과하랬더니, 사과하는 마음으로 업무에 임하고 있다고? #@이름#는? 머리숙여 사과하고 별짓 다했는데 결국 영창 보냈잖아? 이럼 저 인간은? 고개도 못 숙여, 직접 사고도 못하면? 사형시켜야겠네?\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 이놈들은, Offset: (0, 4), Label: O\n",
            "Token: 말장난, Offset: (5, 8), Label: B\n",
            "Token: ##의, Offset: (8, 9), Label: O\n",
            "Token: 연금술, Offset: (10, 13), Label: O\n",
            "Token: ##사들이, Offset: (13, 16), Label: O\n",
            "Token: ##냐, Offset: (16, 17), Label: O\n",
            "Token: ?, Offset: (17, 18), Label: O\n",
            "Token: 직접, Offset: (19, 21), Label: O\n",
            "Token: 머리, Offset: (22, 24), Label: O\n",
            "Token: 숙여, Offset: (25, 27), Label: O\n",
            "Token: 사과, Offset: (28, 30), Label: O\n",
            "Token: ##하, Offset: (30, 31), Label: O\n",
            "Token: ##랬, Offset: (31, 32), Label: O\n",
            "Token: ##더니, Offset: (32, 34), Label: O\n",
            "Token: ,, Offset: (34, 35), Label: O\n",
            "Token: 사과하는, Offset: (36, 40), Label: O\n",
            "Token: 마음으로, Offset: (41, 45), Label: O\n",
            "Token: 업무, Offset: (46, 48), Label: O\n",
            "Token: ##에, Offset: (48, 49), Label: O\n",
            "Token: 임하, Offset: (50, 52), Label: O\n",
            "Token: ##고, Offset: (52, 53), Label: O\n",
            "Token: 있다고, Offset: (54, 57), Label: O\n",
            "Token: ?, Offset: (57, 58), Label: O\n",
            "Token: #, Offset: (59, 60), Label: O\n",
            "Token: @, Offset: (60, 61), Label: O\n",
            "Token: 이름, Offset: (61, 63), Label: O\n",
            "Token: #, Offset: (63, 64), Label: O\n",
            "Token: 는, Offset: (64, 65), Label: O\n",
            "Token: ?, Offset: (65, 66), Label: O\n",
            "Token: 머리, Offset: (67, 69), Label: O\n",
            "Token: ##숙, Offset: (69, 70), Label: O\n",
            "Token: ##여, Offset: (70, 71), Label: O\n",
            "Token: 사과하고, Offset: (72, 76), Label: O\n",
            "Token: 별짓, Offset: (77, 79), Label: O\n",
            "Token: 다, Offset: (80, 81), Label: O\n",
            "Token: ##했는데, Offset: (81, 84), Label: O\n",
            "Token: 결국, Offset: (85, 87), Label: O\n",
            "Token: 영창, Offset: (88, 90), Label: O\n",
            "Token: 보냈, Offset: (91, 93), Label: O\n",
            "Token: ##잖아, Offset: (93, 95), Label: O\n",
            "Token: ?, Offset: (95, 96), Label: O\n",
            "Token: 이럼, Offset: (97, 99), Label: O\n",
            "Token: 저, Offset: (100, 101), Label: O\n",
            "Token: 인간은, Offset: (102, 105), Label: O\n",
            "Token: ?, Offset: (105, 106), Label: O\n",
            "Token: 고개, Offset: (107, 109), Label: O\n",
            "Token: ##도, Offset: (109, 110), Label: O\n",
            "Token: 못, Offset: (111, 112), Label: O\n",
            "Token: 숙여, Offset: (113, 115), Label: O\n",
            "Token: ,, Offset: (115, 116), Label: O\n",
            "Token: 직접, Offset: (117, 119), Label: O\n",
            "Token: 사고, Offset: (120, 122), Label: O\n",
            "Token: ##도, Offset: (122, 123), Label: O\n",
            "Token: 못하면, Offset: (124, 127), Label: O\n",
            "Token: ?, Offset: (127, 128), Label: O\n",
            "Token: 사형시켜야, Offset: (129, 134), Label: O\n",
            "Token: ##겠네, Offset: (134, 136), Label: O\n",
            "Token: ?, Offset: (136, 137), Label: O\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n",
            "Text: 남주여주들은 뭐햤냐 ㅅㅂ\n",
            "Tokens, Offsets, and BIO Labels:\n",
            "Token: [CLS], Offset: (0, 0), Label: O\n",
            "Token: 남, Offset: (0, 1), Label: O\n",
            "Token: ##주, Offset: (1, 2), Label: O\n",
            "Token: ##여, Offset: (2, 3), Label: O\n",
            "Token: ##주들, Offset: (3, 5), Label: O\n",
            "Token: ##은, Offset: (5, 6), Label: O\n",
            "Token: 뭐, Offset: (7, 8), Label: O\n",
            "Token: ##햤, Offset: (8, 9), Label: O\n",
            "Token: ##냐, Offset: (9, 10), Label: O\n",
            "Token: ㅅㅂ, Offset: (11, 13), Label: B\n",
            "Token: [SEP], Offset: (0, 0), Label: O\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤하게 인덱스 뽑아서 확인해보니 태깅이 잘 되어 있는 것으로 확인함."
      ],
      "metadata": {
        "id": "l17Ncg1jJUt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraForTokenClassification\n",
        "\n",
        "model_name = \"beomi/KcELECTRA-base-v2022\"\n",
        "model = ElectraForTokenClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUPKVIZ4EAev",
        "outputId": "87ca72d5-e21d-4584-d7c6-c69e6a523896"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraForTokenClassification(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(54343, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_bio\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs_bio',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJylf7HlECel",
        "outputId": "d3d6db16-3655-48e0-ed76-ff6452a444f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-10-d4927f92b9a0>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "57tblAxtEEvx",
        "outputId": "310433c4-6021-47d4-eaf6-8b7343e379fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241127_052934-39z96d6t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cosmic4dev-yonsei-university/huggingface/runs/39z96d6t' target=\"_blank\">./results_bio</a></strong> to <a href='https://wandb.ai/cosmic4dev-yonsei-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cosmic4dev-yonsei-university/huggingface' target=\"_blank\">https://wandb.ai/cosmic4dev-yonsei-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cosmic4dev-yonsei-university/huggingface/runs/39z96d6t' target=\"_blank\">https://wandb.ai/cosmic4dev-yonsei-university/huggingface/runs/39z96d6t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32280' max='32280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32280/32280 1:54:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.008900</td>\n",
              "      <td>0.016494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>0.016998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.020310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=32280, training_loss=0.007739237065356608, metrics={'train_runtime': 6878.0916, 'train_samples_per_second': 75.09, 'train_steps_per_second': 4.693, 'total_flos': 7.828398001624109e+16, 'train_loss': 0.007739237065356608, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/yaife/detector_bio/saved_model\"\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(\"Model & Tokenizer saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goG8brJMLES-",
        "outputId": "90407974-2614-409e-fa3f-c2fcaef0d099"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model & Tokenizer saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bio_model(model, data_loader, tokenizer, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds.flatten())\n",
        "            all_labels.extend(labels.flatten())\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"O\", \"B\", \"I\"]))\n",
        "\n",
        "evaluate_bio_model(model, DataLoader(test_dataset, batch_size=16), tokenizer, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XF6ohV7EHzk",
        "outputId": "e772cf6a-e623-4f0b-d3d0-7b564ad347fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       1.00      1.00      1.00   1846465\n",
            "           B       0.66      0.63      0.65      9066\n",
            "           I       0.61      0.47      0.53      4469\n",
            "\n",
            "    accuracy                           0.99   1860000\n",
            "   macro avg       0.76      0.70      0.72   1860000\n",
            "weighted avg       0.99      0.99      0.99   1860000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_bio_spans_from_text(text, predictions, tokens, offsets):\n",
        "\n",
        "    harmful_spans = []\n",
        "    current_span = \"\"\n",
        "    current_offsets = None\n",
        "\n",
        "    for token, pred, (start, end) in zip(tokens, predictions, offsets):\n",
        "        # 서브워드 접두어 제거\n",
        "        if token.startswith(\"##\"):\n",
        "            token = token[2:]\n",
        "\n",
        "        if pred == 1:  # B 태그\n",
        "            # 현재 스팬이 존재하면 저장\n",
        "            if current_span:\n",
        "                harmful_spans.append((current_span, current_offsets))\n",
        "            current_span = token\n",
        "            current_offsets = (start, end)\n",
        "\n",
        "        elif pred == 2 and current_span:  # I 태그\n",
        "            # 스팬 이어붙이기\n",
        "            current_span += token\n",
        "            current_offsets = (current_offsets[0], end)\n",
        "\n",
        "        else:  # O 태그\n",
        "            # 현재 스팬 저장 후 초기화\n",
        "            if current_span:\n",
        "                harmful_spans.append((current_span, current_offsets))\n",
        "                current_span = \"\"\n",
        "                current_offsets = None\n",
        "\n",
        "    # 마지막 스팬 저장\n",
        "    if current_span:\n",
        "        harmful_spans.append((current_span, current_offsets))\n",
        "\n",
        "    return harmful_spans\n"
      ],
      "metadata": {
        "id": "6CHqL16HJeHW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from transformers import ElectraForTokenClassification, ElectraTokenizerFast\n",
        "\n",
        "load_path = \"/content/drive/MyDrive/yaife/detector_bio/saved_model\"\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model = ElectraForTokenClassification.from_pretrained(load_path)\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained(load_path)\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlIhQ4NpyIjr",
        "outputId": "31e774b0-8d81-482c-94ae-5caf13b19b07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraForTokenClassification(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(54343, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_bio_spans_from_text(text, predictions, tokens, offsets):\n",
        "\n",
        "    harmful_spans = []\n",
        "    current_span = \"\"\n",
        "    current_offsets = None\n",
        "\n",
        "    for token, pred, (start, end) in zip(tokens, predictions, offsets):\n",
        "        # 서브워드 접두어 제거\n",
        "        if token.startswith(\"##\"):\n",
        "            token = token[2:]\n",
        "\n",
        "        if pred == 1:  # B 태그\n",
        "            # 현재 스팬이 존재하면 저장\n",
        "            if current_span:\n",
        "                harmful_spans.append((current_span, current_offsets))\n",
        "            current_span = token\n",
        "            current_offsets = (start, end)\n",
        "\n",
        "        elif pred == 2 and current_span:  # I 태그\n",
        "            # 스팬 이어붙이기\n",
        "            current_span += token\n",
        "            current_offsets = (current_offsets[0], end)\n",
        "\n",
        "        else:  # O 태그\n",
        "            # 현재 스팬 저장 후 초기화\n",
        "            if current_span:\n",
        "                harmful_spans.append((current_span, current_offsets))\n",
        "                current_span = \"\"\n",
        "                current_offsets = None\n",
        "\n",
        "    # 마지막 스팬 저장\n",
        "    if current_span:\n",
        "        harmful_spans.append((current_span, current_offsets))\n",
        "\n",
        "    return harmful_spans\n"
      ],
      "metadata": {
        "id": "hyD_CvC0hABI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test0\n",
        "# 입력 텍스트\n",
        "input_text = \"너는 정말 재수없어. 한남이야.\"\n",
        "\n",
        "# 텍스트 전처리\n",
        "inputs = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_offsets_mapping=True\n",
        ").to(device)\n",
        "\n",
        "offsets = inputs.pop(\"offset_mapping\")[0].tolist()  # Offset 추출\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])  # 토큰 리스트\n",
        "\n",
        "# 모델 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze().cpu().numpy()  # BIO 태그 예측\n",
        "\n",
        "# BIO 스팬 추출\n",
        "harmful_spans = extract_bio_spans_from_text(input_text, predictions, tokens, offsets)\n",
        "\n",
        "print(\"Input Text:\", input_text)\n",
        "print(\"Harmful Spans:\", harmful_spans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLucgy-f0wV9",
        "outputId": "4b0ac231-3294-4b35-ba0a-cc952c9a7968"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: 너는 정말 재수없어. 한남이야.\n",
            "Harmful Spans: [('한남', (12, 14))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test1\n",
        "# 입력 텍스트\n",
        "input_text = \"그 때 처빨았던 과거의 행적이 욕을 먹는 건데 익1은 뭔 개소리하니\"\n",
        "\n",
        "# 텍스트 전처리\n",
        "inputs = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_offsets_mapping=True\n",
        ").to(device)\n",
        "\n",
        "offsets = inputs.pop(\"offset_mapping\")[0].tolist()  # Offset 추출\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])  # 토큰 리스트\n",
        "\n",
        "# 모델 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze().cpu().numpy()  # BIO 태그 예측\n",
        "\n",
        "# BIO 스팬 추출\n",
        "harmful_spans = extract_bio_spans_from_text(input_text, predictions, tokens, offsets)\n",
        "\n",
        "print(\"Input Text:\", input_text)\n",
        "print(\"Harmful Spans:\", harmful_spans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPZfifYogrp1",
        "outputId": "64ddf3bf-a46c-4be8-8f72-56bf6c7a3dc9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: 그 때 처빨았던 과거의 행적이 욕을 먹는 건데 익1은 뭔 개소리하니\n",
            "Harmful Spans: [('처', (4, 5)), ('빨', (5, 6)), ('개소리', (32, 35))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "개선된 메소드 정의 (유해 단어를 단일 스팬으로 묶기)"
      ],
      "metadata": {
        "id": "p_3jvnXeiDUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_bio_spans_from_text2(text, predictions, tokens, offsets):\n",
        "    spans = []\n",
        "    current_span = None\n",
        "    for token, pred, offset in zip(tokens, predictions, offsets):\n",
        "        if pred == 1:  # \"B\" 태그\n",
        "            if current_span:  # 이전 스팬을 저장\n",
        "                spans.append(current_span)\n",
        "            current_span = [token, offset]\n",
        "        elif pred == 2 and current_span:  # \"I\" 태그\n",
        "            current_span[0] += token.replace(\"##\", \"\")  # 토큰 이어붙임\n",
        "            current_span[1] = (current_span[1][0], offset[1])  # 범위 확장\n",
        "        else:\n",
        "            if current_span:  # 현재 스팬을 종료\n",
        "                spans.append(current_span)\n",
        "                current_span = None\n",
        "    if current_span:\n",
        "        spans.append(current_span)\n",
        "\n",
        "    # 스팬을 텍스트와 범위로 변환\n",
        "    return [(span[0], tuple(span[1])) for span in spans]\n"
      ],
      "metadata": {
        "id": "ioUTuMd3iLkH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test1\n",
        "# 입력 텍스트\n",
        "input_text = \"그 때 처빨았던 과거의 행적이 욕을 먹는 건데 익1은 뭔 개소리하니\"\n",
        "\n",
        "# 텍스트 전처리\n",
        "inputs = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_offsets_mapping=True\n",
        ").to(device)\n",
        "\n",
        "offsets = inputs.pop(\"offset_mapping\")[0].tolist()  # Offset 추출\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])  # 토큰 리스트\n",
        "\n",
        "# 모델 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze().cpu().numpy()  # BIO 태그 예측\n",
        "\n",
        "# BIO 스팬 추출\n",
        "harmful_spans = extract_bio_spans_from_text2(input_text, predictions, tokens, offsets)\n",
        "\n",
        "print(\"Input Text:\", input_text)\n",
        "print(\"Harmful Spans:\", harmful_spans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXbprWeIid7w",
        "outputId": "b0e5ae79-d064-4d0c-a89c-7c2542b54fdb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: 그 때 처빨았던 과거의 행적이 욕을 먹는 건데 익1은 뭔 개소리하니\n",
            "Harmful Spans: [('처', (4, 5)), ('##빨', (5, 6)), ('개소리', (32, 35))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "딱히 기대했던 효과는 아직 안 나타남.."
      ],
      "metadata": {
        "id": "ZJcTvD7VjMFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 함수 정의\n",
        "def preprocess_and_predict(text, model, tokenizer, device):\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_offsets_mapping=True\n",
        "    ).to(device)\n",
        "\n",
        "    offsets = inputs.pop(\"offset_mapping\")[0].tolist()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    return tokens, offsets, predictions\n"
      ],
      "metadata": {
        "id": "gTIqy_afjO2O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "input_text = \"그 때 처빨았던 과거의 행적이 욕을 먹는 건데 익1은 뭔 개소리하니\"\n",
        "\n",
        "tokens, offsets, predictions = preprocess_and_predict(input_text, model, tokenizer, device)\n",
        "\n",
        "harmful_spans = extract_bio_spans_from_text2(input_text, predictions, tokens, offsets)\n",
        "\n",
        "print(\"Input Text:\", input_text)\n",
        "print(\"Harmful Spans:\", harmful_spans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_AdGd-ojd95",
        "outputId": "5883876b-d874-4a81-843d-f59ae695dae3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: 그 때 처빨았던 과거의 행적이 욕을 먹는 건데 익1은 뭔 개소리하니\n",
            "Harmful Spans: [('처', (4, 5)), ('##빨', (5, 6)), ('개소리', (32, 35))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pR7YsVNjji0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}